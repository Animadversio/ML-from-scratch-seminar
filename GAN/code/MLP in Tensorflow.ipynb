{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP with Tensorflow\n",
    "### Data generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "def sample_data(r = 2, n = 1024, sigma = 0.35):\n",
    "    n1 = np.ceil(n/2).astype(int)\n",
    "    n2 = n-n1\n",
    "    theta1 = np.random.uniform(low=0.0, high=2*np.pi, size=(n1,))\n",
    "    theta2 = np.random.uniform(low=0.0, high=2*np.pi, size=(n2,))\n",
    "    radius1 = np.random.randn(n1) * sigma + r\n",
    "    radius2 = np.random.randn(n2) * 2*sigma \n",
    "    x1 = radius1*np.cos(theta1)\n",
    "    y1 = radius1*np.sin(theta1)\n",
    "    x2 = radius2*np.cos(theta2)\n",
    "    y2 = radius2*np.sin(theta2)\n",
    "    x = np.concatenate((np.stack((x1,y1),axis = 1),np.stack((x2,y2),axis = 1)),axis=0)\n",
    "    label = np.concatenate((np.ones((n1,)),np.zeros((n2,))),axis=0)\n",
    "    return x, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data\n",
    "x, label = sample_data()\n",
    "\n",
    "plt.plot(x[label==1,0],x[label==1,1],'r.', markersize = 3)\n",
    "plt.plot(x[label==0,0],x[label==0,1],'b.', markersize = 3)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build MLP discriminator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(X,units=[8,8],reuse=False):\n",
    "    with tf.variable_scope(\"Discriminator\", reuse=reuse): # context manager, so all variables below will be grouped with a suffix \"Discriminator\"\n",
    "        # Specify all variables\n",
    "        # tf.get_variable() gets existing variable with the parameters or creates a new ones; used for weight sharing\n",
    "        w1 = tf.get_variable('w1', shape = [2, units[0]], dtype = tf.float32, initializer = tf.glorot_uniform_initializer)\n",
    "        b1 = tf.get_variable('b1', shape = [1, units[0]], dtype = tf.float32)\n",
    "        w2 = tf.get_variable('w2', shape = [units[0], units[1]], dtype = tf.float32)\n",
    "        b2 = tf.get_variable('b2', shape = [1, units[1]], dtype = tf.float32)\n",
    "        w3 = tf.get_variable('w3', shape = [units[1], 1], dtype = tf.float32)\n",
    "        b3 = tf.get_variable('b3', shape = [1, 1], dtype = tf.float32)\n",
    "        \n",
    "        # Build the MLP\n",
    "        h1 = tf.nn.relu(tf.matmul(X,w1)+b1)\n",
    "        h2 = tf.nn.relu(tf.matmul(h1,w2)+b2)\n",
    "        out = tf.matmul(h2,w3)+b3\n",
    "        \n",
    "    return out # Return the linear output because the loss function takes logits as input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or use Tensorflow built-in layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(X,units=[8,8],reuse=False):\n",
    "    with tf.variable_scope(\"Discriminator\", reuse=reuse): # context manager, so all variables below will be grouped with a suffix \"Discriminator\"\n",
    "        h1 = tf.layers.dense(X,units[0],activation=tf.nn.relu)\n",
    "        h2 = tf.layers.dense(h1,units[1],activation=tf.nn.relu)\n",
    "        out = tf.layers.dense(h2,1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some other preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1. / (1. + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a grid of points for visualizing discriminator\n",
    "grid = np.arange(-3.5, 3.5, 0.05)\n",
    "xv, yv = np.meshgrid(grid, grid)\n",
    "v = np.concatenate((xv.reshape(-1,1),yv.reshape(-1,1)),axis= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Use \"placeholder\" as to contruct empty tensors that receive inputs (X and y)\n",
    "X = tf.placeholder(tf.float32,[None,2])  # None for undetermined number: flexible sample size\n",
    "y = tf.placeholder(tf.float32,[None,1])\n",
    "\n",
    "# 2. Set the grid points into tensorflow constant\n",
    "V = tf.constant(v, dtype = tf.float32)\n",
    "\n",
    "# 3. Pass X through discriminator to obtain logits (linear output)\n",
    "s_logits = discriminator(X)\n",
    "\n",
    "# 4. Pass V through discriminator to obtain logits \n",
    "# Note: This is the second time using variables in discriminator, so set reuse = True\n",
    "v_logits = discriminator(V, reuse=True)\n",
    "\n",
    "# 5. Write down loss function\n",
    "# Here I use tensorflow built-in function tf.nn.sigmoid_cross_entropy_with_logits, but one can write the loss function from scratch\n",
    "# This built-in function deals with numerical instability, see line 109 of the source code\n",
    "# https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/ops/nn_impl.py#L108-L185\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=s_logits,labels=y))\n",
    "\n",
    "# 6. Identify variables for optimization\n",
    "disc_vars = tf.trainable_variables(scope=\"Discriminator\")\n",
    "\n",
    "# 7. Set up optimizer (other options: GradientDescentOptimizer, MomentumOptimizer, AdamOptimizer, etc.)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=0.001)\n",
    "\n",
    "# 8. Use \"minimize\" function to optimize loss by changing the weights of selected variables\n",
    "opt = optimizer.minimize(loss,var_list = disc_vars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the training for-loop in a Tensorflow session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set up batch size and number of epoch\n",
    "batch_size = 1024\n",
    "epochs = 2000\n",
    "acc_list = []\n",
    "loss_list = []\n",
    "\n",
    "# Run the graph with a tensorflow session\n",
    "with tf.Session() as sess:\n",
    "    # At the beginning of the session, run this line to initialize\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Loop over epochs\n",
    "    for i in range(epochs):\n",
    "        # Sample from data generator\n",
    "        X_batch, label = sample_data(n=batch_size)\n",
    "\n",
    "        # Run one iteration using \"run\" on optimization step\n",
    "        # (1) Query variables of interest by calling them in a list with the optimization\n",
    "        # (2) Use \"feed_dict\" to feed in samples\n",
    "        _, this_loss, s_log, v_log = sess.run([opt, loss, s_logits, v_logits], feed_dict={X: X_batch, y: label.reshape(-1,1)})\n",
    "        \n",
    "        # Collect loss and accuracy\n",
    "        loss_list.append(this_loss)\n",
    "        acc = np.mean((sigmoid(s_log)>0.5)==label.reshape(-1,1))\n",
    "        acc_list.append(acc)\n",
    "\n",
    "        # Visualization\n",
    "        if i%100 == 0 or i == epochs-1:                        \n",
    "            v_pred = sigmoid(v_log).reshape(xv.shape[0], yv.shape[0])\n",
    "            extent = [-3.525, 3.525, -3.525, 3.525]\n",
    "                       \n",
    "            plt.clf()\n",
    "            plt.imshow(v_pred, extent = extent, alpha = 0.2, cmap = 'bwr')\n",
    "            plt.plot(X_batch[label==1,0],X_batch[label==1,1],'r.', markersize=2)\n",
    "            plt.plot(X_batch[label==0,0],X_batch[label==0,1],'b.', markersize=2)\n",
    "            plt.xlim([-3.5,3.5])\n",
    "            plt.ylim([-3.5,3.5])\n",
    "            plt.title(\"Iter: %d \\nloss= %.4f, acc= %.4f\"%(i,this_loss,acc));\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(plt.gcf())\n",
    "            time.sleep(0.5)\n",
    "\n",
    "plt.clf();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.plot(loss_list,linewidth = 1)\n",
    "plt.ylim([0,1])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training losses');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy\n",
    "plt.plot(acc_list,linewidth = 1)\n",
    "plt.ylim([0,1])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training accuracy');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset graph\n",
    "# tf.reset_default_graph()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
