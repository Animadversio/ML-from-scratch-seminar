{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance Sampling and Particle filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import poisson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance Sampling and resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we dive into the vast universe of nonlinear filtering, let us take a step back and review importance sampling. The idea of importance is to empirically approximate a probability distribution, which we can evaluate but cannot directly sample from, by weighted samples from _another_ probability distribution. \n",
    "As an example, let us consider a mysterious parabola-shaped probability function $p(x) =  3/4 * (1-x^2) $, defined on $x $ in $ [-1,1]$. We cannot sample from this distribution, but we can sample from the uniform distribution between -1 and 1. Let us now generate weighted samples to approximate the parabola distribution (algorithm 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximate beta distribution with weighted samples from the uniform distribution\n",
    "np.random.seed(42)\n",
    "N = 1000\n",
    "\n",
    "# draw samples from proposal\n",
    "# %%%%%%%%%%%%%%% ENTER CODE HERE %%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "# compute weights\n",
    "# %%%%%%%%%%%%%%% ENTER CODE HERE %%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "# normalize the weights such that they sum to 1\n",
    "# %%%%%%%%%%%%%%% ENTER CODE HERE %%%%%%%%%%%%%%%%%%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of original samples\n",
    "fig, (ax1,ax2) = plt.subplots(2,1,figsize=(10,8))\n",
    "fig.tight_layout()\n",
    "ax1.hist(x)\n",
    "ax1.set_title('Original samples')\n",
    "\n",
    "# histogram of weighted samples, together with probability distribution\n",
    "bins = np.linspace(-1, 1, int(np.sqrt(N)))\n",
    "ax2.hist(x,weights=w,density = True,alpha=0.5)\n",
    "xaxis = np.linspace(-1,1,50)\n",
    "ax2.plot(xaxis,f(xaxis)/(4/3))\n",
    "ax2.legend([r'$p(x)$','weighted histogram'])\n",
    "ax2.set_title('Weighted samples')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume you don't want to store the weights for some reason, but want to have samples that represent the parabola distribution. In this case, you can use the resampling algorithm (algorithm 4) to generate equally weighted samples from the weight distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample(x,w,N):\n",
    "    # %%%%%%%%%%%%%%% ENTER CODE HERE %%%%%%%%%%%%%%%%%%%\n",
    "    return x_r,w_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resample to produce equally weighted samples, which is equivalent to samples without the need for a weight\n",
    "x_r,_ = resample(x,w,N)\n",
    "\n",
    "# histogram of original samples\n",
    "fig, ax2 = plt.subplots(1,1,figsize=(10,4))\n",
    "fig.tight_layout()\n",
    "\n",
    "# histogram of weighted samples\n",
    "bins = np.linspace(-1, 1, int(np.sqrt(N)))\n",
    "ax2.hist(x,weights=w,density = True,alpha=0.5)\n",
    "ax2.hist(x_r,density = True,alpha=0.5)\n",
    "xaxis = np.linspace(-1,1,50)\n",
    "ax2.plot(xaxis,f(xaxis)/(4/3))\n",
    "ax2.legend([r'$p(x)$','weighted histogram','resampled histogram'])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that even though the idea of resampling is intruguingly simple, it will increase the variance of the samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started with particle filters: Revisit the random walk\n",
    "In order to compare the PF (and to benchmark it), let us use it on the simple random walk model that we have encountered in the KF section. We already have an optimal solution to this problem (the Kalman filter), and we will now compare the PF to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KF1D_generateData(params):\n",
    "    \n",
    "    x = np.zeros(params[\"T\"]+1)\n",
    "    y = np.zeros(params[\"T\"]+1)\n",
    "    \n",
    "    # initialization\n",
    "    x[0] = np.random.normal(params[\"mu0\"],np.sqrt(params[\"Sigma0\"]))\n",
    "    y[0] = np.random.normal(params[\"H\"] * x[0], np.sqrt(params[\"R\"]))\n",
    "    \n",
    "    for i in range(1,params[\"T\"]+1):\n",
    "        x[i] = np.random.normal(params[\"F\"] * x[i-1],np.sqrt(params[\"Q\"]))\n",
    "        y[i] = np.random.normal(params[\"H\"] * x[i], np.sqrt(params[\"R\"]))\n",
    "    \n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KF1D(y,params):\n",
    "    \n",
    "    # %%%%%%%%%%%%%%% COPY CODE HERE %%%%%%%%%%%%%%%%%%%\n",
    "            \n",
    "    \n",
    "    return mu, Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "N = 2000\n",
    "c = 0.5\n",
    "\n",
    "params = {\n",
    "    \"F\": 1,\n",
    "    \"Q\": 1,\n",
    "    \"H\": 1,\n",
    "    \"R\": 10,\n",
    "    \"mu0\": 10,\n",
    "    \"Sigma0\": 2,\n",
    "    \"T\": 100\n",
    "}\n",
    "\n",
    "# generate the data\n",
    "x, y = KF1D_generateData(params)\n",
    "\n",
    "# compute the KF estimate\n",
    "mu, Sigma = KF1D(y,params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's code up the Boostrap Particle filter (Algorithm 5). You may use the resampling procedure that you have already defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BootstrapPF(y,N,c,params):\n",
    "    \n",
    "    # initialization\n",
    "    # %%%%%%%%%%%%%%% ENTER CODE HERE %%%%%%%%%%%%%%%%%%%\n",
    "    \n",
    "    # filtering recursion\n",
    "    for t in range(1,params[\"T\"]+1):\n",
    "        # draw from proposal (transition density)\n",
    "        # %%%%%%%%%%%%%%% ENTER CODE HERE %%%%%%%%%%%%%%%%%%%\n",
    "        #compute weights\n",
    "        # %%%%%%%%%%%%%%% ENTER CODE HERE %%%%%%%%%%%%%%%%%%%\n",
    "        # normalize weights\n",
    "        # %%%%%%%%%%%%%%% ENTER CODE HERE %%%%%%%%%%%%%%%%%%%\n",
    "        \n",
    "        # resample if necessary\n",
    "        # %%%%%%%%%%%%%%% ENTER CODE HERE %%%%%%%%%%%%%%%%%%%\n",
    "    return x, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# compute the Bootstrap PF estimate\n",
    "x_PF,w_PF = BootstrapPF(y,N,c,params)\n",
    "\n",
    "# compute mean and variance\n",
    "mu_PF = np.sum(w_PF * x_PF,1)\n",
    "Sigma_PF = np.sum(w_PF * ((x_PF.T-mu_PF)**2).T,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the trajectory and the observations\n",
    "# (assume no observation at y = 0 )\n",
    "\n",
    "t = np.arange(params[\"T\"]+1)\n",
    "\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(2,1,figsize=(10,8))\n",
    "\n",
    "ax1.scatter(t[1:],y[1:],color='grey',facecolors='none')\n",
    "ax1.scatter(t,x,color='black')\n",
    "ax1.plot(t,mu,linewidth=2)\n",
    "ax1.plot(t,mu_PF,'--',linewidth=2)\n",
    "ax1.legend([r'$\\mu_t$ (KF)',r'$\\mu_t$ (PF)',r'$y_t$',r'$x_t$'])\n",
    "ax1.grid(True)\n",
    "ax1.set_xlim(0,params[\"T\"])\n",
    "ax1.set_title('Mean')\n",
    "\n",
    "ax2.plot(t,Sigma)\n",
    "ax2.plot(t,Sigma_PF)\n",
    "ax2.set_xlim(0,params[\"T\"])\n",
    "ax2.set_title('Variance')\n",
    "ax2.legend([r'$\\Sigma_t$ (KF)',r'$\\Sigma_t$ (PF)'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear Gaussian models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we consider a nonlinear filtering task, where the hidden state $X_t$ evolves according to a drift-diffusion with nonlinear drift function. Further, the observations $Y_t$ are linear and corrupted by Gaussian noise. Specifically, the generative model in terms of stochastic differential equations (SDE) reads:\n",
    "\\begin{eqnarray}\n",
    "d X_t & = & \\tilde{f}(X_t) \\, dt + \\sigma_x \\, dW_t \\\\\n",
    "d Z_t & = & \\tilde{h}(X_t) \\, dt + \\sigma_y \\, dV_t,\n",
    "\\end{eqnarray}\n",
    "with $ \\tilde{f}(x) = -4x(x^2-1) $ and $h(x) = x $.\n",
    "\n",
    "Don't worry if you have never worked with SDE's before. It's actually nothing else than the dynamical system we looked at in the KF notebook, just written in a slightly weird way. Analogously, they can easily be discretized in time, and we can rewrite the model in terms of the following transition and emission probabilities:\n",
    "\\begin{eqnarray}\n",
    " p(x_t | x_{t-1} ) & = & \\mathcal{N} ( x_t ; f(x_{t-1}), Q) \\\\\n",
    "p(y_t | x_{t} ) & = & \\mathcal{N} ( z_t ; h(x_{t}) , R),\n",
    "\\end{eqnarray}\n",
    "with\n",
    "\\begin{eqnarray}\n",
    "f(x) & = & x + \\tilde{f}(x)  dt \\\\\n",
    "Q & = & \\sigma_x^2 dt \\\\\n",
    "h(x)  & = & \\tilde{h}(x) dt \\\\\n",
    "R & = & \\sigma_y^2 / dt.\n",
    "\\end{eqnarray}\n",
    "The observations $y_t$ can be considered a temporal derivative of the process $Z_t$ (don't tell the mathematicians I said that). Note that again the observation variance scales inversely with the time step: The smaller I make the time step, the less informative a single observation becomes. On the other hand, I also have more observations per time, so this prevents oversampling my observations and thus making the inference trivial.\n",
    "\n",
    "This is a nonlinear model, and we cannot use the Kalman filter anymore. In other words: Whatever the particle filter is doing, we have to trust it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLGauss_generateData(params):\n",
    "    \n",
    "    # unpack some parameters for readibility\n",
    "    f = params[\"f\"]\n",
    "    h = params[\"h\"]\n",
    "    \n",
    "    x = np.zeros(params[\"T\"]+1)\n",
    "    y = np.zeros(params[\"T\"]+1)\n",
    "    \n",
    "    # initialization (draw from Gauss with mean mu0 and variance Sigma0)\n",
    "    x[0] = np.random.normal(params[\"mu0\"],np.sqrt(params[\"Sigma0\"]))\n",
    "    \n",
    "    for t in range(1,params[\"T\"]+1):\n",
    "        x[t] = np.random.normal( f(x[t-1])  , np.sqrt(params[\"Q\"]) )\n",
    "        y[t] = np.random.normal(h(x[t]), np.sqrt(params[\"R\"]) )\n",
    "    \n",
    "    \n",
    "    return x, y\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "N = 2000 # number of particles \n",
    "c = 0.2 # resampling criterion\n",
    "dt = 0.001\n",
    "\n",
    "params = {\n",
    "    \"f\": lambda x: x-4*x*(x**2-1)*dt,\n",
    "    \"Q\": 2 * dt,\n",
    "    \"h\": lambda x: x,\n",
    "    \"R\": 0.1 / dt,\n",
    "    \"mu0\": 0,\n",
    "    \"Sigma0\": 1,\n",
    "    \"T\": int(5/dt),\n",
    "}\n",
    "\n",
    "x,y = NLGauss_generateData(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you code the BPF, you can actually re-use a lot of the code from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLGauss_BPF(y,N,c,params):\n",
    "    \n",
    "    # %%%%%%%%%%%%%%% COPY CODE HERE, ADJUST %%%%%%%%%%%%%%%%%%%\n",
    "            \n",
    "    return x, w\n",
    "\n",
    "np.random.seed(42)\n",
    "x_PF,w_PF = NLGauss_BPF(y,N,c,params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now visualize the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produces weighted histogram images\n",
    "def histImage(x, bins, rang, w=0):\n",
    "    image = np.zeros((x.shape[0],bins))\n",
    "    \n",
    "    if np.isscalar(w):\n",
    "        for i in range(x.shape[0]):\n",
    "            image[i,:] = np.histogram(x[i,:],bins,rang,density=True)[0]\n",
    "    else:\n",
    "        for i in range(x.shape[0]):\n",
    "            image[i,:] = np.histogram(x[i,:],bins,rang,weights=w[i,:],density=True)[0]\n",
    "        \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = params[\"T\"]\n",
    "plotrange = [0,T,-2.5,2.5]\n",
    "t = np.arange(T+1)*dt\n",
    "\n",
    "fig, (ax2) = plt.subplots(1,1,figsize=(10,6))\n",
    "\n",
    "\n",
    "\n",
    "hist = np.transpose(histImage(x_PF,int(np.sqrt(N)),(-3,3),w=w_PF))\n",
    "ax2.imshow(np.flipud(hist), cmap='Oranges', interpolation='nearest', extent=[0,T,-3,3],aspect='auto',vmax=0.7)\n",
    "ax2.plot(t,x,color='xkcd:moss')\n",
    "ax2.plot(t,np.average(x_PF,1,w_PF), linewidth=3,color = 'xkcd:azure')\n",
    "ax2.axis(plotrange)\n",
    "ax2.legend(['hidden state','BPF'],fontsize=16)\n",
    "ax2.legend([r'$x_t$',r'$\\mu_t$ (BPF)'])\n",
    "\n",
    "\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "plt.savefig('63 - particle filters - gauss.pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear filtering with Poisson noise\n",
    "As an alternative to Gaussian-type observation noise, we consider here point-process observations, with the intensity $ g(x_t)$ being a function of the latent state $x_t$.\n",
    "\\begin{eqnarray}\n",
    "y_t &\\sim & Poisson(g(x_t)).\n",
    "\\end{eqnarray}\n",
    "As a concrete example, we consider a Gaussian-shaped rate function $ g(x) = g_0 \\exp(\\frac{x-m_o}{2 s_0^2}) dt $ for two sensors with peaks at $ m_0 = \\pm 1 $ and width $ s_0 $ (i.e. conditionally independent two-dimensional observations).\n",
    "The hidden dynamics is the same as in the previous example. If you want to draw the link to neuroscience, you might consider those \"sensors\" to be two place cells that fire with a higher rate once the animal (the latent state) is close to their respective place fields. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLPoisson_generateData(params):\n",
    "    \n",
    "    # unpack some parameters for readibility\n",
    "    f = params[\"f\"]\n",
    "    g = params[\"g\"]\n",
    "    \n",
    "    x = np.zeros(params[\"T\"]+1)\n",
    "    y = np.zeros((params[\"T\"]+1,2))\n",
    "    \n",
    "    # initialization (draw from Gauss with mean mu0 and variance Sigma0)\n",
    "    x[0] = np.random.normal(params[\"mu0\"],np.sqrt(params[\"Sigma0\"]))\n",
    "    \n",
    "    for t in range(1,params[\"T\"]+1):\n",
    "        x[t] = np.random.normal( f(x[t-1])  , np.sqrt(params[\"Q\"]) )\n",
    "        y[t] = np.random.poisson(g(x[t]))\n",
    "    \n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "N = 2000\n",
    "c = 0.2\n",
    "dt = 0.001\n",
    "g0 = 50\n",
    "s0 = 0.05\n",
    "m0 = np.array([-1,1])\n",
    "\n",
    "params = {\n",
    "    \"f\": lambda x: x-4*x*(x**2-1)*dt,\n",
    "    \"Q\": 2 * dt,\n",
    "    \"g\": lambda x: g0 * np.transpose(np.exp( - np.array([x-m0[0],x-m0[1]])**2/(2 * s0**2) ))*dt,\n",
    "    \"mu0\": 0,\n",
    "    \"Sigma0\": 1,\n",
    "    \"T\": int(5/dt),\n",
    "}\n",
    "\n",
    "x,y = NLPoisson_generateData(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as before, just code up the BPF. Careful: The weighting step requires a bit of thinking..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLPoisson_BPF(y,N,c,params):\n",
    "    \n",
    "    # %%%%%%%%%%%%%%% COPY CODE HERE, ADJUST %%%%%%%%%%%%%%%%%%%\n",
    "            \n",
    "    return x, w\n",
    "\n",
    "x_PF_PP,w_PF_PP = NLPoisson_BPF(y,N,c,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_minus = np.where(y[:,0]>=1)[0]\n",
    "t_plus = np.where(y[:,1]>=1)[0]\n",
    "T = params[\"T\"]*dt\n",
    "t = np.arange(T/dt+1)*dt\n",
    "\n",
    "plotrange = [0, T, -2.5, 2.5]\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(2,1,figsize=(7.5,8))\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "ax1.plot(t,x,color = 'xkcd:moss')\n",
    "ax1.plot(t,np.average(x_PF_PP,1,w_PF_PP),color = 'xkcd:azure',linewidth=3)\n",
    "for spikepos in t[np.where(y>=1)[0]]:\n",
    "    ax1.axvline(x=spikepos,linestyle='--',color = 'xkcd:light grey',linewidth=1)\n",
    "ax1.scatter(t[t_plus],2.2*np.ones(t_plus.size),marker=\"^\",c='xkcd:eggplant purple',s=100)\n",
    "ax1.scatter(t[t_minus],-2.2*np.ones(t_minus.size),marker=\"v\",c='xkcd:eggplant purple',s=100)\n",
    "ax1.axis(plotrange)\n",
    "ax1.legend(['hidden state','PF'],fontsize=16)\n",
    "\n",
    "hist = np.transpose(histImage(x_PF_PP,int(np.sqrt(N)),(-3,3),w=w_PF_PP))\n",
    "ax2.imshow(np.flipud(hist), cmap='Oranges', interpolation='nearest', extent=[0,T,-3,3],aspect='auto',vmax=0.7)\n",
    "for spikepos in t[np.where(y>=1)[0]]:\n",
    "    ax2.axvline(x=spikepos,linestyle='--',color = 'xkcd:light grey',linewidth=1)\n",
    "ax2.scatter(t[t_plus],2.2*np.ones(t_plus.size),marker=\"^\",c='xkcd:eggplant purple',s=100)\n",
    "ax2.scatter(t[t_minus],-2.2*np.ones(t_minus.size),marker=\"v\",c='xkcd:eggplant purple',s=100)\n",
    "ax2.plot(t,x,color='xkcd:moss')\n",
    "ax2.plot(t,np.average(x_PF,1,w_PF), linewidth=3,color = 'xkcd:azure')\n",
    "ax2.axis(plotrange)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
