{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing the necessary packages\n",
    "\n",
    "(this is also found in the `setup.md` file)\n",
    "\n",
    "It is recommended to use [anaconda](https://docs.conda.io/en/latest/miniconda.html) to manage python environments. Follow the installation instructions to use anaconda.\n",
    "\n",
    "Create a new evironment: `conda create -n rl python=3.7`\n",
    "\n",
    "Make sure you change your conda environment to the new `rl` environment you created above: `conda activate rl`\n",
    "\n",
    "Then install Open AI's [gym package](http://gym.openai.com/docs/): `pip install gym`\n",
    "\n",
    "Install jupyter to use notebooks: `pip install jupyter`\n",
    "\n",
    "Make sure you also install these: `pip install numpy seaborn matplotlib`\n",
    "\n",
    "Clone this reinforcement learning repository: `git clone https://github.com/wingillis/reinforcement-learning.git`. Make note of the location you saved this repository, (i.e., use `pwd` to get the full folder path pointing to this repository) because you will use this location in our notebooks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Link](http://gym.openai.com) to more documentation\n",
    "\n",
    "[Link](http://gym.openai.com/envs/#atari) to some default environments\n",
    "\n",
    "`gym` is a package developed by Open AI with the goal of providing reproducible and complex\n",
    "environments to benchmark different reinforcement learning algorithms.\n",
    "The goal is to minimize the amount of knowledge the RL agent has in how to interact\n",
    "with its environment, and the gym API reflects that. \n",
    "\n",
    "It contains default many environments, including:\n",
    "\n",
    "- simple text-based environments, like `Taxi-v3`, or `Blackjack-v0`\n",
    "- simple graphical environments, such as `CartPole-v1`, and `Pendulum-v0`\n",
    "- Atari games, such as `Breakout-v0`\n",
    "\n",
    "It's easy to extend the environment interface and make your own. Today we're going\n",
    "to use a custom environment that reflects the Sutton and Barto textbook.\n",
    "\n",
    "It is very simple to use. The main components of gym allows you to:\n",
    "\n",
    "- specify which environment you want to use\n",
    "- render the environment\n",
    "- take an action\n",
    "\n",
    "You can define any function you like, and use any python package you want\n",
    "(e.g. tensorflow, pytorch, numpy, autograd) to decide which actions\n",
    "to take given the current state of the environment.\n",
    "\n",
    "Let's set up an example here to understand the basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "\n",
    "# add the reinforcement learning repository we downloaded to your system path\n",
    "sys.path.append('/Users/wingillis/dev/reinforcement-learning')\n",
    "\n",
    "# import a basic gridworld environment\n",
    "from lib.envs.gridworld import GridworldEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are instantiating a new gridworld environment, containing two reward locations:\n",
    "one in the top left corner, and one in the bottom right corner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the environment\n",
    "env = GridworldEnv(shape=(5, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every time you want to begin a new episode of training, you'll want to reset the environment.\n",
    "\n",
    "Here, resetting the environment will place the agent in a random spot in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "# you can deterministically start in the same place by setting the seed.\n",
    "env.seed(1)\n",
    "\n",
    "position = env.reset()\n",
    "print(position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's render the current environment.\n",
    "\n",
    "Here, the two `T` symbols represent the goals, while the `x`\n",
    "in the bottom left corner represents the agent's position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T  o  o  o  o\n",
      "o  o  o  o  o\n",
      "o  o  o  o  o\n",
      "o  o  o  o  o\n",
      "x  o  o  o  T\n",
      "\r"
     ]
    }
   ],
   "source": [
    "env.render('human')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how you get the number of actions and states the current environment supports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# actions: 4\n",
      "# states: 25\n"
     ]
    }
   ],
   "source": [
    "print('# actions:', env.nA)\n",
    "print('# states:', env.nS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's perform an action. In the gridworld environment, we can perform\n",
    "1 of 4 different actions:\n",
    "\n",
    "- 0: up\n",
    "- 1: right\n",
    "- 2: down\n",
    "- 3: left\n",
    "\n",
    "When we perform an action, we get back an observation, reward, an indicator\n",
    "saying if we've reached the goal, and any extra information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Run one timestep of the environment's dynamics. When end of\n",
       "episode is reached, you are responsible for calling `reset()`\n",
       "to reset this environment's state.\n",
       "\n",
       "Accepts an action and returns a tuple (observation, reward, done, info).\n",
       "\n",
       "Args:\n",
       "    action (object): an action provided by the agent\n",
       "\n",
       "Returns:\n",
       "    observation (object): agent's observation of the current environment\n",
       "    reward (float) : amount of reward returned after previous action\n",
       "    done (bool): whether the episode has ended, in which case further step() calls will return undefined results\n",
       "    info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)\n",
       "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/rl/lib/python3.7/site-packages/gym/envs/toy_text/discrete.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.step?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T  o  o  o  o\n",
      "o  o  o  o  o\n",
      "o  o  o  o  o\n",
      "x  o  o  o  o\n",
      "o  o  o  o  T\n",
      "\r"
     ]
    }
   ],
   "source": [
    "next_state, rew, done, info = env.step(0)  # let's move up one\n",
    "env.render('human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 -1.0 False {'prob': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(next_state, rew, done, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this specific environment, you can look at the entire model of the environment, by accessing the `P` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0: [(1.0, 0, 0.0, True)],\n",
       "  1: [(1.0, 0, 0.0, True)],\n",
       "  2: [(1.0, 0, 0.0, True)],\n",
       "  3: [(1.0, 0, 0.0, True)]},\n",
       " 1: {0: [(1.0, 1, -1.0, False)],\n",
       "  1: [(1.0, 2, -1.0, False)],\n",
       "  2: [(1.0, 6, -1.0, False)],\n",
       "  3: [(1.0, 0, -1.0, True)]},\n",
       " 2: {0: [(1.0, 2, -1.0, False)],\n",
       "  1: [(1.0, 3, -1.0, False)],\n",
       "  2: [(1.0, 7, -1.0, False)],\n",
       "  3: [(1.0, 1, -1.0, False)]},\n",
       " 3: {0: [(1.0, 3, -1.0, False)],\n",
       "  1: [(1.0, 4, -1.0, False)],\n",
       "  2: [(1.0, 8, -1.0, False)],\n",
       "  3: [(1.0, 2, -1.0, False)]},\n",
       " 4: {0: [(1.0, 4, -1.0, False)],\n",
       "  1: [(1.0, 4, -1.0, False)],\n",
       "  2: [(1.0, 9, -1.0, False)],\n",
       "  3: [(1.0, 3, -1.0, False)]},\n",
       " 5: {0: [(1.0, 0, -1.0, True)],\n",
       "  1: [(1.0, 6, -1.0, False)],\n",
       "  2: [(1.0, 10, -1.0, False)],\n",
       "  3: [(1.0, 5, -1.0, False)]},\n",
       " 6: {0: [(1.0, 1, -1.0, False)],\n",
       "  1: [(1.0, 7, -1.0, False)],\n",
       "  2: [(1.0, 11, -1.0, False)],\n",
       "  3: [(1.0, 5, -1.0, False)]},\n",
       " 7: {0: [(1.0, 2, -1.0, False)],\n",
       "  1: [(1.0, 8, -1.0, False)],\n",
       "  2: [(1.0, 12, -1.0, False)],\n",
       "  3: [(1.0, 6, -1.0, False)]},\n",
       " 8: {0: [(1.0, 3, -1.0, False)],\n",
       "  1: [(1.0, 9, -1.0, False)],\n",
       "  2: [(1.0, 13, -1.0, False)],\n",
       "  3: [(1.0, 7, -1.0, False)]},\n",
       " 9: {0: [(1.0, 4, -1.0, False)],\n",
       "  1: [(1.0, 9, -1.0, False)],\n",
       "  2: [(1.0, 14, -1.0, False)],\n",
       "  3: [(1.0, 8, -1.0, False)]},\n",
       " 10: {0: [(1.0, 5, -1.0, False)],\n",
       "  1: [(1.0, 11, -1.0, False)],\n",
       "  2: [(1.0, 15, -1.0, False)],\n",
       "  3: [(1.0, 10, -1.0, False)]},\n",
       " 11: {0: [(1.0, 6, -1.0, False)],\n",
       "  1: [(1.0, 12, -1.0, False)],\n",
       "  2: [(1.0, 16, -1.0, False)],\n",
       "  3: [(1.0, 10, -1.0, False)]},\n",
       " 12: {0: [(1.0, 7, -1.0, False)],\n",
       "  1: [(1.0, 13, -1.0, False)],\n",
       "  2: [(1.0, 17, -1.0, False)],\n",
       "  3: [(1.0, 11, -1.0, False)]},\n",
       " 13: {0: [(1.0, 8, -1.0, False)],\n",
       "  1: [(1.0, 14, -1.0, False)],\n",
       "  2: [(1.0, 18, -1.0, False)],\n",
       "  3: [(1.0, 12, -1.0, False)]},\n",
       " 14: {0: [(1.0, 9, -1.0, False)],\n",
       "  1: [(1.0, 14, -1.0, False)],\n",
       "  2: [(1.0, 19, -1.0, False)],\n",
       "  3: [(1.0, 13, -1.0, False)]},\n",
       " 15: {0: [(1.0, 10, -1.0, False)],\n",
       "  1: [(1.0, 16, -1.0, False)],\n",
       "  2: [(1.0, 20, -1.0, False)],\n",
       "  3: [(1.0, 15, -1.0, False)]},\n",
       " 16: {0: [(1.0, 11, -1.0, False)],\n",
       "  1: [(1.0, 17, -1.0, False)],\n",
       "  2: [(1.0, 21, -1.0, False)],\n",
       "  3: [(1.0, 15, -1.0, False)]},\n",
       " 17: {0: [(1.0, 12, -1.0, False)],\n",
       "  1: [(1.0, 18, -1.0, False)],\n",
       "  2: [(1.0, 22, -1.0, False)],\n",
       "  3: [(1.0, 16, -1.0, False)]},\n",
       " 18: {0: [(1.0, 13, -1.0, False)],\n",
       "  1: [(1.0, 19, -1.0, False)],\n",
       "  2: [(1.0, 23, -1.0, False)],\n",
       "  3: [(1.0, 17, -1.0, False)]},\n",
       " 19: {0: [(1.0, 14, -1.0, False)],\n",
       "  1: [(1.0, 19, -1.0, False)],\n",
       "  2: [(1.0, 24, -1.0, True)],\n",
       "  3: [(1.0, 18, -1.0, False)]},\n",
       " 20: {0: [(1.0, 15, -1.0, False)],\n",
       "  1: [(1.0, 21, -1.0, False)],\n",
       "  2: [(1.0, 20, -1.0, False)],\n",
       "  3: [(1.0, 20, -1.0, False)]},\n",
       " 21: {0: [(1.0, 16, -1.0, False)],\n",
       "  1: [(1.0, 22, -1.0, False)],\n",
       "  2: [(1.0, 21, -1.0, False)],\n",
       "  3: [(1.0, 20, -1.0, False)]},\n",
       " 22: {0: [(1.0, 17, -1.0, False)],\n",
       "  1: [(1.0, 23, -1.0, False)],\n",
       "  2: [(1.0, 22, -1.0, False)],\n",
       "  3: [(1.0, 21, -1.0, False)]},\n",
       " 23: {0: [(1.0, 18, -1.0, False)],\n",
       "  1: [(1.0, 24, -1.0, True)],\n",
       "  2: [(1.0, 23, -1.0, False)],\n",
       "  3: [(1.0, 22, -1.0, False)]},\n",
       " 24: {0: [(1.0, 24, 0.0, True)],\n",
       "  1: [(1.0, 24, 0.0, True)],\n",
       "  2: [(1.0, 24, 0.0, True)],\n",
       "  3: [(1.0, 24, 0.0, True)]}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take some random actions and see how our agent moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action():\n",
    "  return random.randint(0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x  o  o  o  o\n",
      "o  o  o  o  o\n",
      "o  o  o  o  o\n",
      "o  o  o  o  o\n",
      "o  o  o  o  T\n",
      "\r"
     ]
    }
   ],
   "source": [
    "n_steps = 30\n",
    "for t in range(n_steps):\n",
    "  action = select_action()\n",
    "  _ = env.step(action)\n",
    "  clear_output()\n",
    "  env.render('human')\n",
    "  time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Reinforcement learning",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
